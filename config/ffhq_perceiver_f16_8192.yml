model:
  vqgan:
    target: vqgan.models.vqgan.VQModel
    checkpoint_path: ./runs/ffhq_256_f16_8192/vqgan_209114.pt
    params:
      codebook:
        target: vector_quantize_pytorch.VectorQuantize
        params:
          dim: 1024
          codebook_size: 8192
      encoder_decoder:
        encoder_target: vqgan.modules.encoders.encoders.MultiLayerEncoder2D
        decoder_target: vqgan.modules.decoders.decoders.MultiLayerDecoder2D
        params:
          resolution: 256
          in_channels: 3 # RGB
          out_channels: 3 # RGB
          attention_target: vqgan.modules.attention.AttnBlock
          channels: 128 # first convolution out dim
          channel_multiplier: [ 1, 1, 2, 2, 4 ] # by how much to multiply the channel for each successive layer
          z_channels: 256 # the last layer
          num_res_blocks: 2 # number of residual blocks per layer
          resolution_attention: [16]
  perceiver:
    params:
#      vocab_size: 8192 # same as codebook size + coord vocab (4096)
#      block_size: 512 # dimension of the embedding + coordinates
#      n_layer: 24 # number of layers
#      n_head: 16
#      n_embd: 1024
#      embd_pdrop: 0.1
#      resid_pdrop: 0.1
#      attn_pdrop: 0.1


training:
  save_every: 5000
  sample_every: 100
  batch_size: 1
  learning_rate: 3e-4

data:
  target: utils.data.PerceiverLatentsDataset
  params:
    data_dir: ../data/ffhq_256_f16_8192_latents/