model:
  target: model.unet.DiffusionModel
  params:
    timestep_embed: 16
    num_classes: 0
    size: [64, 64]
    in_channels: 1
    out_channels: 1
    base_hidden_channels: 64
    n_layers: 4
    chan_multiplier: [1, 2, 2, 4]
    inner_layers: [2, 3, 3, 4]
    attention_layers: [False, False, True, True]

diffusion:
  target: model.diffusion.GaussianDiffusion
  params:
    image_size: 64
    timesteps: 1000

vqgan:
    target: vqgan.models.vqgan.VQModel
    checkpoint_path: ./vqgan/runs/ffhq_f16_8192/vqgan_173748.pt
    params:
      codebook:
        target: vector_quantize_pytorch.VectorQuantize
        params:
          dim: 1024
          codebook_size: 8192
      encoder_decoder:
        encoder_target: vqgan.modules.encoders.encoders.MultiLayerEncoder2D
        decoder_target: vqgan.modules.decoders.decoders.MultiLayerDecoder2D
        params:
          resolution: 256
          in_channels: 3 # RGB
          out_channels: 3 # RGB
          attention_target: vqgan.modules.attention.AttnBlock
          channels: 128 # first convolution out dim
          channel_multiplier: [1, 1, 2, 2, 4] # by how much to multiply the channel for each successive layer
          z_channels: 256 # the last layer
          num_res_blocks: 2 # number of residual blocks per layer
          resolution_attention: [16]



training:
  learning_rate: 1e-5
  batch_size: 128

data:
  target: utils.data.ImageLatentsDataset
  params:
    root: ./data/ffhq_f16_8192_latents