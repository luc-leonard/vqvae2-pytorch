model:
  vqgan:
    target: vqgan.models.vqgan.VQModel
    checkpoint_path: ./runs/ffhq_f16_8192/vqgan_173748.pt
    params:
      codebook:
        target: vector_quantize_pytorch.VectorQuantize
        params:
          dim: 1024
          codebook_size: 8192
      encoder_decoder:
        encoder_target: vqgan.modules.encoders.encoders.MultiLayerEncoder2D
        decoder_target: vqgan.modules.decoders.decoders.MultiLayerDecoder2D
        params:
          resolution: 256
          in_channels: 3 # RGB
          out_channels: 3 # RGB
          attention_target: vqgan.modules.attention.AttnBlock
          channels: 128 # first convolution out dim
          channel_multiplier: [1, 1, 2, 2, 4] # by how much to multiply the channel for each successive layer
          z_channels: 256 # the last layer
          num_res_blocks: 2 # number of residual blocks per layer
          resolution_attention: [16]
  gpt:
    params:
      vocab_size: 8192 # same as codebook size + coord vocab (4096)
      block_size: 4096 # dimension of the embedding + coordinates
      n_layer: 12 # number of layers
      n_head: 8
      n_embd: 512
      embd_pdrop: 0.1
      resid_pdrop: 0.1
      attn_pdrop: 0.1


training:
  save_every: 5000
  sample_every: 100
  batch_size: 8
  learning_rate: 3e-5

data:
  target: utils.data.ImageLatentsDataset
  params:
    data_dir: ../data/ffhq_f16_8192_latents/
