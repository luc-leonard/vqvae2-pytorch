model:
  vqgan:
    target: vqgan.models.vqgan.VQModel
    checkpoint_path: ./runs/ffhq_256_gan/vqgan_376000.pt
    params:
      codebook:
        target: vector_quantize_pytorch.VectorQuantize
        params:
          dim: 512
          codebook_size: 1024
      encoder_decoder:
        encoder_target: vqgan.modules.encoders.encoders.MultiLayerEncoder2D
        decoder_target: vqgan.modules.decoders.decoders.MultiLayerDecoder2D
        params:
          resolution: 256
          in_channels: 3 # RGB
          out_channels: 3 # RGB
          attention_target: vqgan.modules.attention.AttnBlock
          channels: 128 # first convolution out dim
          channel_multiplier: [1, 2, 2, 4] # by how much to multiply the channel for each successive layer
          z_channels: 256 # the last layer
          num_res_blocks: 2 # number of residual blocks per layer
          resolution_attention: [32]
  gpt:
    params:
      vocab_size: 1024 # same as codebook size
      block_size: 1024 # dimension of the embedding
      n_layer: 12
      n_head: 8
      n_embd: 512
      embd_pdrop: 0.0
      resid_pdrop: 0.0
      attn_pdrop: 0.0

training:
  save_every: 1000
  sample_every: 100
  batch_size: 16
  learning_rate: 3e-4

data:
  target: utils.data.LatentsDataset
  params:
    data_dir: ../data/ffhq_vqgan_latents
