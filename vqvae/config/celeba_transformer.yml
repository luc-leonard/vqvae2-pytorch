model:
  vqvae:
    params:
      dimension: 2
      embedding_dim: 128
      codebook_size: 512
      input_dim: 3 # RGB

      hidden_dim: 256 # first convolution out dim
      num_residual_layers: 8 # number of residual blocks per layer
      dim_residual_layers: 32 # dimension of residual blocks
  gpt:
    params:
      vocab_size: 512 # same as codebook size
      block_size: 1024 # dimension of the embedding
      n_layer: 12
      n_head: 8
      n_embd: 512
      embd_pdrop: 0.0
      resid_pdrop: 0.0
      attn_pdrop: 0.0

training:
  learning_rate: 3e-4
  save_every: 100
  sample_every: 100

data:
  dir: ../data/celeba_latents
  batch_size: 16
