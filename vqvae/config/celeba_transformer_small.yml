model:
  vqvae:
    params:
      embedding_dim: 128
      codebook_size: 512
      input_dim: 3 # RGB

      hidden_dim: 256 # first convolution out dim
      num_residual_layers: 8 # number of residual blocks per layer
      dim_residual_layers: 32 # dimension of residual blocks
  gpt:
    params:
      vocab_size: 512 # same as codebook size
      block_size: 1024 # dimension of the embedding
      n_layer: 2
      n_head: 2
      n_embd: 256
      embd_pdrop: 0.0
      resid_pdrop: 0.0
      attn_pdrop: 0.0

training:
  learning_rate: 0.03
  generate_every: 1000
  save_every: 1000

data:
  dir: ../data/celeba_latents
  batch_size: 1024
